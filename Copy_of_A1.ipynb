{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment One (EECS3405 F24)**\n",
        "\n",
        "**your name (your student number): Luca Stefanescu 219004852** \\\\\n",
        "**your yorku email: luca1@my.yorku.ca**\n",
        "\n",
        "This assignment is mainly for you to review mathematical background. You have to work individually. Remember to fill in your information (name, student number, email) at above.\n",
        "\n",
        "\n",
        "\n",
        "##**What to Submit**\n",
        "\n",
        "Please use this notebook to complete assignment one. You have to run your codes and show the results in this notebook. Download the completed notenook as `.ipynb` and compress it as a `.zip` file to submit to eClass.  Submit only ONE notebook file that contains all of your answers and codes to eClass before the deadline.  No late submission\n",
        "will be accepted.\n",
        "\n",
        "* For all written parts, write your answers in text cells. To avoid confusions in marking, better to embed latex codes there to represent all mathematical notations and equations.  **No handwriting is accepted**.\n",
        "\n",
        "* For programming parts, you should give codes, comments, explanations and the proper running outputs in both code and text cells. Make your jupyter notebook clean and concise. Remove all unused codes and all intermediate results from the submitted notebook. The submitted notebook should include only the final (best) outputs for each question. Also make sure every code cell runnable so that markers can reproduce the outputs if necessary."
      ],
      "metadata": {
        "id": "3EU9nI8EWbrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1** (10 marks)  ###\n",
        "\n",
        "Given two sets of $m$ vectors, $\\mathbf{x}_i \\in \\mathbb{R}^n$  and $\\mathbf{y}_i \\in \\mathbb{R}^n$ for all $i=1,2, \\cdots, m$, verify that the summation $\\sum_{i=1}^m  \\mathbf{x}_i  \\mathbf{y}_i^\\intercal$ can be vectorized as the following matrix multiplication:\n",
        "$$\n",
        "\\sum_{i=1}^m \\mathbf{x}_i \\mathbf{y}_i^\\intercal = \\mathbf{X}  \\mathbf{Y}^\\intercal,\n",
        "$$\n",
        "where $\\mathbf{X} = \\big[ \\mathbf{x}_1 \\, \\mathbf{x}_2 \\, \\cdots \\, \\mathbf{x}_m \\big] \\in \\mathbb{R}^{n\\times m}$ and $\\mathbf{Y} = \\big[ \\mathbf{y}_1 \\, \\mathbf{y}_2 \\, \\cdots \\, \\mathbf{y}_m \\big] \\in \\mathbb{R}^{n\\times m}$."
      ],
      "metadata": {
        "id": "E1OwmDh6Zv_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*write your answer to Q1 here:*"
      ],
      "metadata": {
        "id": "rIWKLT7taeil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.** We WTS:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^m \\mathbf{x}_i \\mathbf{y}_i^\\intercal = \\mathbf{X}  \\mathbf{Y}^\\intercal \\quad(1)\n",
        "$$\n",
        "\n",
        "Starting with the LHS, this can be expanded as:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^m \\mathbf{x}_i \\mathbf{y}_i^\\intercal = \\mathbf{x}_1 \\mathbf{y}_1^\\intercal + \\mathbf{x}_2 \\mathbf{y}_2^\\intercal + \\ldots + \\mathbf{x}_m \\mathbf{y}_m^\\intercal\n",
        "$$\n",
        "\n",
        "Now, going back to (1) we will expand the RHS:\n",
        "$$\n",
        "\\mathbf{X} \\mathbf{Y}^T =\n",
        "\\begin{bmatrix}\n",
        "x_1 & x_2 & \\cdots & x_m\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_m\n",
        "\\end{bmatrix}^T\n",
        "= \\mathbf{x}_1 \\mathbf{y}_1^\\intercal + \\mathbf{x}_2 \\mathbf{y}_2^\\intercal + \\ldots + \\mathbf{x}_m \\mathbf{y}_m^\\intercal\n",
        "$$\n",
        "Both sides are equal therefore the proof is complete.\n"
      ],
      "metadata": {
        "id": "OiLu8IE9omrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q2** (10 marks)  ###\n",
        "\n",
        "Given $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{z} \\in \\mathbb{R}^m$, and\n",
        "$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ ($m < n$), compute the following two partial derivatives:\n",
        "\n",
        "1.   $\\frac{\\partial}{\\partial \\mathbf{z}} \\, \\Vert  \\mathbf{A} \\mathbf{x} - \\mathbf{z} \\Vert^2$\n",
        "\n",
        "2.   $\\frac{\\partial}{\\partial \\mathbf{x}} \\, \\Vert  \\mathbf{A} \\mathbf{x} - \\mathbf{z} \\Vert^2$\n",
        "\n"
      ],
      "metadata": {
        "id": "RgoTthU7aoV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "   $$\n",
        "   f(x, z) = \\|Ax - z\\|^2 = (Ax - z)^T (Ax - z)\n",
        "   $$\n",
        "\n",
        "   Compute the partial derivative with respect to z:\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial}{\\partial z} f(x, z) = \\frac{\\partial}{\\partial z} (Ax - z)^T (Ax - z)\n",
        "   $$\n",
        "\n",
        "   Chain rule\n",
        "\n",
        "   $$\n",
        "   = -2(Ax - z)\n",
        "   $$\n",
        "   $$\n",
        "   \\\\[12pt]\n",
        "   $$\n",
        "2.\n",
        "   $$\n",
        "   \\frac{\\partial}{\\partial x} f(x, z) = \\frac{\\partial}{\\partial x} (Ax - z)^T (Ax - z)\n",
        "   $$\n",
        "\n",
        "   Using the chain rule:\n",
        "\n",
        "   $$\n",
        "   = 2(Ax - z)^T \\frac{\\partial}{\\partial x} (Ax - z)\n",
        "   $$\n",
        "\n",
        "   The derivative of Ax with respect to x is:\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial}{\\partial x} (Ax) = A\n",
        "   $$\n",
        "   $$\n",
        "   \\\\\n",
        "   $$\n",
        "   $$\n",
        "   = 2(Ax - z)^T A\n",
        "   $$\n",
        "\n",
        "   we get:\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial}{\\partial x} f(x, z) = 2A^T (Ax - z)\n",
        "   $$\n"
      ],
      "metadata": {
        "id": "kId5VtjBgTAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q3** (20 marks) ###\n",
        "\n",
        "**Part 3.1 (10 marks):** Derive a gradient decent algorithm to solve the following quadratic optimization problem:\n",
        "\n",
        "$$\n",
        "    \\min_{\\mathbf{x}} \\;\\; \\frac12 \\mathbf{x}^\\intercal \\mathbf{Q} \\mathbf{x} - \\mathbf{1}^\\intercal \\mathbf{x}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{x} \\in \\mathbb{R}^n$ is free variables, $\\mathbf{1}$ is a constant $n$-dimensional vector whose elements are all 1,  and $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ is given as a constant definite positive matrix ($\\mathbf{Q} \\succ 0$)."
      ],
      "metadata": {
        "id": "2jw48ZfUexHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*for Part 3.1, derive your gradient descent algorithm here:*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dXRhRXQCfLLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Compute the derivative of the objective function:\n",
        "   \n",
        "   Using the definition of the gradient, we differentiate $f(\\mathbb{x})$:\n",
        "\n",
        "   - The derivative of $\\frac{1}{2} \\mathbb{x}^T Q \\mathbb{x} $ is:\n",
        "\n",
        "   $$\n",
        "   \\nabla \\left( \\frac{1}{2} \\mathbb{x}^T Q \\mathbb{x} \\right) = Q \\mathbb{x}\n",
        "   $$\n",
        "\n",
        "   - The derivative of $-\\mathbb{1}^T \\mathbb{x}$ is:\n",
        "\n",
        "   $$\n",
        "   \\nabla \\left( -\\mathbb{1}^T \\mathbb{x} \\right) = -\\mathbb{1}\n",
        "   $$\n",
        "\n",
        "2. Combine the results to find the gradient:\n",
        "\n",
        "   $$\n",
        "   \\nabla f(\\mathbb{x}) = Q \\mathbb{x} - \\mathbb{1}\n",
        "   $$\n",
        "\n",
        "Gradient Descent algorithm\n",
        "\n",
        "1. Initialize $\\mathbb{x}_0$.\n",
        "2. Repeat for number of epochs\n",
        "   - Compute the gradient: $\\nabla f(\\mathbb{x}_k) = Q \\mathbb{x}_k - \\mathbb{1} $.\n",
        "   - Update: $\\mathbb{x}_{k+1} = \\mathbb{x}_k - \\alpha (Q \\mathbb{x}_k - \\mathbb{1}) $.\n"
      ],
      "metadata": {
        "id": "Q62KtxQGhKE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3.2 (10 marks):**  Use `numpy` to write a gradient descent code to solve the above quadratic optimization problem.  Print the solution $\\mathbf{x}^*$ for the following $\\mathbf{Q}$ matrix."
      ],
      "metadata": {
        "id": "z3mUDnDlfchg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "Q = np.array([[13.57599411,-7.77916099,4.45073215,0.18157029,2.55400279,-1.06271637,-0.14237939,-4.73200647],\n",
        " [-7.77916099,9.16482133,1.349321,0.4697293,-3.29959602,3.41477056,-4.66924506,1.74397411],\n",
        " [4.45073215,1.349321,11.66150687,4.7431563,4.6278791,0.61248024,-2.52679826,-4.94462079],\n",
        " [0.18157029,0.4697293,4.7431563,13.50647727,8.16622632,2.77908368,-2.36985429,-7.53962758],\n",
        " [2.55400279,-3.29959602,4.6278791,8.16622632,12.2258951,-1.84304299,1.04977205,-8.24939242],\n",
        " [-1.06271637,3.41477056,0.61248024,2.77908368,-1.84304299,6.67184174,-4.60482886,0.28920379],\n",
        " [-0.14237939,-4.66924506,-2.52679826,-2.36985429,1.04977205,-4.60482886,13.16925778,3.41548824],\n",
        " [-4.73200647,1.74397411,-4.94462079, -7.53962758,-8.24939242,0.28920379,3.41548824,8.72679051]])\n",
        "\n",
        "print(Q.shape)\n",
        "\n",
        "# calculated that the gradient is grad_f(x) = Q.dot(x) - 1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPl-IOvzemCa",
        "outputId": "83632204-b585-4c2d-83f1-411a0e0efc03"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "Q = np.array([[13.57599411,-7.77916099,4.45073215,0.18157029,2.55400279,-1.06271637,-0.14237939,-4.73200647],\n",
        " [-7.77916099,9.16482133,1.349321,0.4697293,-3.29959602,3.41477056,-4.66924506,1.74397411],\n",
        " [4.45073215,1.349321,11.66150687,4.7431563,4.6278791,0.61248024,-2.52679826,-4.94462079],\n",
        " [0.18157029,0.4697293,4.7431563,13.50647727,8.16622632,2.77908368,-2.36985429,-7.53962758],\n",
        " [2.55400279,-3.29959602,4.6278791,8.16622632,12.2258951,-1.84304299,1.04977205,-8.24939242],\n",
        " [-1.06271637,3.41477056,0.61248024,2.77908368,-1.84304299,6.67184174,-4.60482886,0.28920379],\n",
        " [-0.14237939,-4.66924506,-2.52679826,-2.36985429,1.04977205,-4.60482886,13.16925778,3.41548824],\n",
        " [-4.73200647,1.74397411,-4.94462079, -7.53962758,-8.24939242,0.28920379,3.41548824,8.72679051]])\n",
        "\n",
        "print(Q.shape)\n",
        "\n",
        "# setting an initial x\n",
        "x = np.ones((Q.shape[0], 1))\n",
        "#matrix of ones\n",
        "ones = np.ones((Q.shape[0], 1))\n",
        "#initial f(x)\n",
        "f_x = 1/2 * x.T.dot(Q).dot(x) - ones.T.dot(x)\n",
        "print(\"Initial f(x): {}\".format(f_x))\n",
        "\n",
        "def gradient_descent(learning_rate, Q, x, epochs):\n",
        "  for i in range(epochs):\n",
        "    gradient = Q.dot(x) - ones\n",
        "    #taking a step\n",
        "    x = x - learning_rate * gradient\n",
        "    #only print f(x) every 10 000'th iteration\n",
        "    if (i % 10000) == 0:\n",
        "      f_x = 1/2 * x.T.dot(Q).dot(x) - ones.T.dot(x)\n",
        "      print(\"updated f(x): {}\".format(f_x))\n",
        "\n",
        "gradient_descent(0.001, Q, x, 100000)"
      ],
      "metadata": {
        "id": "QoKi0-TNgHAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f394eaf8-666f-4b88-b71f-43109c68c3e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 8)\n",
            "Initial f(x): [[22.43541279]]\n",
            "updated f(x): [[21.31053629]]\n",
            "updated f(x): [[-10.69545331]]\n",
            "updated f(x): [[-11.84510139]]\n",
            "updated f(x): [[-12.00918309]]\n",
            "updated f(x): [[-12.0326014]]\n",
            "updated f(x): [[-12.03594374]]\n",
            "updated f(x): [[-12.03642077]]\n",
            "updated f(x): [[-12.03648885]]\n",
            "updated f(x): [[-12.03649857]]\n",
            "updated f(x): [[-12.03649995]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see our f(x) is converging to a value meaning our gradient descent algorithm works."
      ],
      "metadata": {
        "id": "r1sNVRGUe2qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q4** (20 marks)\n",
        "\n",
        "**Part 4.1 (10 marks):** Derive the procedure to compute the distance of a point $\\mathbf{x}_0 \\in \\mathbb{R}^n$ to\n",
        "an elliptic surface $\\mathbf{x}^\\intercal \\mathbf{A} \\mathbf{x} = 1$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite ($\\mathbf{A} \\succ 0$).\n",
        "\n",
        "*Hints: Use the method of Lagrange multipliers to derive the formula, and may need to use a numerical procedure (e.g. grid search) to find out the unknown multiplier(s) if no closed-form solution exists.*\n",
        "\n"
      ],
      "metadata": {
        "id": "Od0psVpKgX8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*for Part 4.1, derive your method here:*"
      ],
      "metadata": {
        "id": "7kDdMYE9gp2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to minimize the distance:\n",
        "\n",
        "$$\n",
        "d^2 = \\min_{\\mathbb{x}} \\| \\mathbb{x_0} - \\mathbb{x} \\|^2\n",
        "$$\n",
        "\n",
        "subject to the constraint:\n",
        "\n",
        "$$\n",
        "\\mathbb{x}^T A \\mathbb{x} = 1.\n",
        "$$\n",
        "\n",
        "To solve this, we define the Lagrangian:\n",
        "\n",
        "$$\n",
        "L(\\mathbb{x}, \\lambda) = \\| \\mathbb{x_0} - \\mathbb{x} \\|^2 + \\lambda (\\mathbb{x}^T A \\mathbb{x} - 1).\n",
        "$$\n",
        "\n",
        "Expanding:\n",
        "\n",
        "$$\n",
        "L(\\mathbb{x}, \\lambda) = (\\mathbb{x} - \\mathbb{x_0})^T (\\mathbb{x} - \\mathbb{x_0}) + \\lambda (\\mathbb{x}^T A \\mathbb{x} - 1).\n",
        "$$\n",
        "\n",
        "Simplifying further:\n",
        "\n",
        "$$\n",
        "L(\\mathbb{x}, \\lambda) = (\\mathbb{x} - \\mathbb{x_0})^T (\\mathbb{x} - \\mathbb{x_0}) + \\lambda \\mathbb{x}^T A \\mathbb{x} - \\lambda.\n",
        "$$\n",
        "\n",
        "Next, we take the derivative of \\( L \\) with respect to $\\mathbb{x}$ and set it to zero:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbb{x}} = 0 \\implies 2(\\mathbb{x} - \\mathbb{x_0}) + 2\\lambda A \\mathbb{x} = 0.\n",
        "$$\n",
        "\n",
        "Rearranging:\n",
        "\n",
        "$$\n",
        "\\mathbb{x_0} = \\lambda A \\mathbb{x} + \\mathbb{x} \\implies \\mathbb{x_0} = (\\lambda A + \\mathbb{I})\\mathbb{x}.\n",
        "$$\n",
        "\n",
        "Thus, solving for $\\mathbb{x}:$\n",
        "\n",
        "$$\n",
        "\\mathbb{x} = (\\lambda A + \\mathbb{I})^{-1} \\mathbb{x_0}.\n",
        "$$\n",
        "\n",
        "Now, plug this value of $\\mathbb{x}:$ back into the constraint: $\\mathbb{x}^T A \\mathbb{x} = 1$:\n",
        "\n",
        "$$\n",
        "\\left[ (\\lambda A + \\mathbb{I})^{-1} \\mathbb{x_0} \\right]^T A \\left[ (\\lambda A + \\mathbb{I})^{-1} \\mathbb{x_0} \\right] = 1.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "AvEXvlwQdmBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4.2 (10 marks):** Use `numpy` to implement a grid search to compute the above distance.\n",
        "Use the following given $\\mathbf{x}_0$ and $\\mathbf{A}$ as an example to print out the distance."
      ],
      "metadata": {
        "id": "9cwqbh2ugrgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[10.82133645, 3.92407517, 0.28991746, -2.03411727, 4.55376106],\n",
        "              [3.92407517, 8.55336068, -0.21915151, 0.42212004, 2.08124307],\n",
        "              [0.28991746, -0.21915151, 5.74622299, -0.800496, 0.63366946],\n",
        "              [-2.03411727, 0.42212004, -0.800496, 3.91232017, -0.44196245],\n",
        "              [4.55376106, 2.08124307, 0.63366946, -0.44196245, 3.83040884]])\n",
        "\n",
        "x0 = np.array([-0.08111786, 1.68122205, 0.84108852, 1.74916213, -0.11266981])\n",
        "\n",
        "def compute_expression(lambda_val, x0, A):\n",
        "    I = np.eye(A.shape[0])\n",
        "    term = np.dot(I - (lambda_val) * A, x0)\n",
        "    return np.dot(term.T, np.dot(A, term))\n",
        "\n",
        "lambda_values = np.arange(1, 10, 0.001)\n",
        "results = []\n",
        "\n",
        "for lambda_val in lambda_values:\n",
        "    value = compute_expression(lambda_val, x0, A)\n",
        "    results.append((lambda_val, value))\n",
        "\n",
        "optimal_lambda = min(results, key=lambda x: x[1])[0]\n",
        "\n",
        "print(f\"Optimal lambda: {optimal_lambda}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97-9gK9shLMy",
        "outputId": "d9f06f7e-5fbc-4a2f-9269-834ce35ab533"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal lambda: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twuSb21ApAU5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}